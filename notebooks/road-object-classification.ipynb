{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6115289,"sourceType":"datasetVersion","datasetId":3504673},{"sourceId":3181265,"sourceType":"datasetVersion","datasetId":1932507}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Road Object Classification \n## Setup for Road Signs Classes\n### Explore the data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/german-traffic-sign-dataset/signname.csv\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:31:46.114615Z","iopub.execute_input":"2025-08-06T11:31:46.114845Z","iopub.status.idle":"2025-08-06T11:31:46.563111Z","shell.execute_reply.started":"2025-08-06T11:31:46.114822Z","shell.execute_reply":"2025-08-06T11:31:46.562451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"ClassId\"].size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:31:46.563826Z","iopub.execute_input":"2025-08-06T11:31:46.564393Z","iopub.status.idle":"2025-08-06T11:31:46.570417Z","shell.execute_reply.started":"2025-08-06T11:31:46.564363Z","shell.execute_reply":"2025-08-06T11:31:46.569536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\nwith open(\"/kaggle/input/german-traffic-sign-dataset/train.p\", 'rb') as f:\n    train_data = pickle.load(f)\nwith open(\"/kaggle/input/german-traffic-sign-dataset/valid.p\", 'rb') as f:\n    val_data = pickle.load(f)\nwith open(\"/kaggle/input/german-traffic-sign-dataset/test.p\", 'rb') as f:\n    test_data = pickle.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:31:46.571808Z","iopub.execute_input":"2025-08-06T11:31:46.572001Z","iopub.status.idle":"2025-08-06T11:31:47.939746Z","shell.execute_reply.started":"2025-08-06T11:31:46.571986Z","shell.execute_reply":"2025-08-06T11:31:47.939067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.keys()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:31:47.940638Z","iopub.execute_input":"2025-08-06T11:31:47.940850Z","iopub.status.idle":"2025-08-06T11:31:47.946041Z","shell.execute_reply.started":"2025-08-06T11:31:47.940834Z","shell.execute_reply":"2025-08-06T11:31:47.945333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data[\"labels\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:31:47.946723Z","iopub.execute_input":"2025-08-06T11:31:47.946953Z","iopub.status.idle":"2025-08-06T11:31:47.958722Z","shell.execute_reply.started":"2025-08-06T11:31:47.946932Z","shell.execute_reply":"2025-08-06T11:31:47.957974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data[\"labels\"].size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:31:47.959384Z","iopub.execute_input":"2025-08-06T11:31:47.959595Z","iopub.status.idle":"2025-08-06T11:31:47.970642Z","shell.execute_reply.started":"2025-08-06T11:31:47.959576Z","shell.execute_reply":"2025-08-06T11:31:47.969902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data[\"sizes\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:31:47.971328Z","iopub.execute_input":"2025-08-06T11:31:47.971566Z","iopub.status.idle":"2025-08-06T11:31:47.982880Z","shell.execute_reply.started":"2025-08-06T11:31:47.971544Z","shell.execute_reply":"2025-08-06T11:31:47.982277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data[\"coords\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:31:47.983790Z","iopub.execute_input":"2025-08-06T11:31:47.984066Z","iopub.status.idle":"2025-08-06T11:31:47.996544Z","shell.execute_reply.started":"2025-08-06T11:31:47.984043Z","shell.execute_reply":"2025-08-06T11:31:47.995795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data[\"features\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:31:47.999296Z","iopub.execute_input":"2025-08-06T11:31:47.999538Z","iopub.status.idle":"2025-08-06T11:31:48.008653Z","shell.execute_reply.started":"2025-08-06T11:31:47.999499Z","shell.execute_reply":"2025-08-06T11:31:48.007841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data[\"features\"][0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:31:48.009561Z","iopub.execute_input":"2025-08-06T11:31:48.010117Z","iopub.status.idle":"2025-08-06T11:31:48.019810Z","shell.execute_reply.started":"2025-08-06T11:31:48.010082Z","shell.execute_reply":"2025-08-06T11:31:48.019121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport cv2\n\n# convert to cv2 BGR format\ndef get_random_img(dataset):\n    random_sample = random.choice(dataset[\"features\"])\n    return cv2.cvtColor(random_sample, cv2.COLOR_RGB2BGR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:31:48.020556Z","iopub.execute_input":"2025-08-06T11:31:48.020747Z","iopub.status.idle":"2025-08-06T11:31:48.248940Z","shell.execute_reply.started":"2025-08-06T11:31:48.020734Z","shell.execute_reply":"2025-08-06T11:31:48.248341Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef show(img, cv2_img = True, size=(6, 4)):\n    if cv2_img: \n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    plt.figure(figsize=size) # img display size\n    plt.imshow(img)\n    plt.axis('off')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T12:22:47.801585Z","iopub.execute_input":"2025-08-06T12:22:47.802269Z","iopub.status.idle":"2025-08-06T12:22:47.806925Z","shell.execute_reply.started":"2025-08-06T12:22:47.802243Z","shell.execute_reply":"2025-08-06T12:22:47.806157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = get_random_img(train_data)\nshow(img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:31:48.254849Z","iopub.execute_input":"2025-08-06T11:31:48.255069Z","iopub.status.idle":"2025-08-06T11:31:48.409020Z","shell.execute_reply.started":"2025-08-06T11:31:48.255053Z","shell.execute_reply":"2025-08-06T11:31:48.408329Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preprocessing","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef process_image(img): \n    # Step 1: Conditional CLAHE\n    # extract luminance channel\n    img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n    luminance = img_yuv[:, :, 0]\n    avg_brightness = np.mean(luminance)\n\n    if avg_brightness < 50:\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        img_yuv[:, :, 0] = clahe.apply(img_yuv[:, :, 0])\n        img_clahe = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n    else:\n        img_clahe = img.copy()\n\n    # Step 2: Gamma Correction\n    lut = np.zeros(256, dtype=np.uint8)\n    for i in range(256):\n        # normalize pixel value to [0, 1]\n        normalized = i / 255.0\n        # adaptive gamma factor (current maximum: 1.5)\n        gamma = 1.0 + (1.5 - 1.0) * (1 - normalized)  # gamma decreases with brightness\n        corrected = pow(normalized, 1.0 / gamma)\n        lut[i] = np.clip(corrected * 255, 0, 255)\n    img_bright = cv2.LUT(img, lut)\n\n\n    # Step 3: Unsharp Masking\n    blurred = cv2.GaussianBlur(img_bright, (0, 0), sigmaX=2)\n    amount = 1.5\n    return cv2.addWeighted(img_bright, 1 + amount, blurred, -amount, 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:31:48.409751Z","iopub.execute_input":"2025-08-06T11:31:48.410556Z","iopub.status.idle":"2025-08-06T11:31:48.416329Z","shell.execute_reply.started":"2025-08-06T11:31:48.410532Z","shell.execute_reply":"2025-08-06T11:31:48.415470Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processed_img = process_image(img)\nshow(img)\nshow(processed_img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:31:48.417083Z","iopub.execute_input":"2025-08-06T11:31:48.417454Z","iopub.status.idle":"2025-08-06T11:31:48.514313Z","shell.execute_reply.started":"2025-08-06T11:31:48.417431Z","shell.execute_reply":"2025-08-06T11:31:48.513601Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For the folder structure I use numerical classes and look the result up in the csv later.","metadata":{}},{"cell_type":"code","source":"base_dir = \"dataset\"\ntrain_dir = os.path.join(base_dir, \"train\")\ntest_dir = os.path.join(base_dir, \"test\")\nval_dir = os.path.join(base_dir, \"val\")\nos.makedirs(base_dir, exist_ok=True)\nos.makedirs(train_dir, exist_ok=True)\nos.makedirs(test_dir, exist_ok=True)\nos.makedirs(val_dir, exist_ok=True)\n\n\nfor i in range(0, df[\"ClassId\"].size):\n    os.makedirs(os.path.join(train_dir, str(i)), exist_ok=True)\n    os.makedirs(os.path.join(test_dir, str(i)), exist_ok=True)\n    os.makedirs(os.path.join(val_dir, str(i)), exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:31:48.515100Z","iopub.execute_input":"2025-08-06T11:31:48.515668Z","iopub.status.idle":"2025-08-06T11:31:48.526939Z","shell.execute_reply.started":"2025-08-06T11:31:48.515642Z","shell.execute_reply":"2025-08-06T11:31:48.526179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm.notebook import tqdm\n\nfor i, img in enumerate(tqdm(train_data[\"features\"], desc=\"Processing Images\")):\n    label = str(train_data[\"labels\"][i])\n    out_path = os.path.join(train_dir, label, f\"{i}_{label}.png\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    cv2.imwrite(out_path, process_image(img))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:31:48.527732Z","iopub.execute_input":"2025-08-06T11:31:48.528291Z","iopub.status.idle":"2025-08-06T11:33:26.848031Z","shell.execute_reply.started":"2025-08-06T11:31:48.528254Z","shell.execute_reply":"2025-08-06T11:33:26.847406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i, img in enumerate(tqdm(val_data[\"features\"], desc=\"Processing Images\")):\n    label = str(val_data[\"labels\"][i])\n    out_path = os.path.join(val_dir, label, f\"{i}_{label}.png\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    cv2.imwrite(out_path, process_image(img))\n\nfor i, img in enumerate(tqdm(test_data[\"features\"], desc=\"Processing Images\")):\n    label = str(test_data[\"labels\"][i])\n    out_path = os.path.join(test_dir, label, f\"{i}_{label}.png\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    cv2.imwrite(out_path, process_image(img))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:33:26.848722Z","iopub.execute_input":"2025-08-06T11:33:26.848944Z","iopub.status.idle":"2025-08-06T11:34:14.757086Z","shell.execute_reply.started":"2025-08-06T11:33:26.848920Z","shell.execute_reply":"2025-08-06T11:34:14.756446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_random_cv2_img(path, only_path=False):\n    files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n    random_sample = random.choice(files)\n    if only_path:\n        return random_sample\n    else:\n        return cv2.imread(os.path.join(path, random_sample))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:34:14.757732Z","iopub.execute_input":"2025-08-06T11:34:14.757951Z","iopub.status.idle":"2025-08-06T11:34:14.762249Z","shell.execute_reply.started":"2025-08-06T11:34:14.757934Z","shell.execute_reply":"2025-08-06T11:34:14.761571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show(get_random_cv2_img(os.path.join(train_dir, \"0\")))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:34:14.762843Z","iopub.execute_input":"2025-08-06T11:34:14.763040Z","iopub.status.idle":"2025-08-06T11:34:14.812971Z","shell.execute_reply.started":"2025-08-06T11:34:14.763025Z","shell.execute_reply":"2025-08-06T11:34:14.812377Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Setup for Traffic Light Classes\n### Explore the data","metadata":{}},{"cell_type":"code","source":"img = get_random_cv2_img(\"/kaggle/input/carla-traffic-lights-images/traffic_light_data/train/green\")\nshow(img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:34:14.813585Z","iopub.execute_input":"2025-08-06T11:34:14.813838Z","iopub.status.idle":"2025-08-06T11:34:15.980490Z","shell.execute_reply.started":"2025-08-06T11:34:14.813817Z","shell.execute_reply":"2025-08-06T11:34:15.979872Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"43 classes, so 0 - 42","metadata":{}},{"cell_type":"code","source":"img.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:34:15.981083Z","iopub.execute_input":"2025-08-06T11:34:15.981272Z","iopub.status.idle":"2025-08-06T11:34:15.985689Z","shell.execute_reply.started":"2025-08-06T11:34:15.981257Z","shell.execute_reply":"2025-08-06T11:34:15.985062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resized_img = cv2.resize(img, (32, 32))\nshow(resized_img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:34:15.986426Z","iopub.execute_input":"2025-08-06T11:34:15.986669Z","iopub.status.idle":"2025-08-06T11:34:16.026275Z","shell.execute_reply.started":"2025-08-06T11:34:15.986653Z","shell.execute_reply":"2025-08-06T11:34:16.025650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show(process_image(resized_img))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:34:16.026923Z","iopub.execute_input":"2025-08-06T11:34:16.027156Z","iopub.status.idle":"2025-08-06T11:34:16.060786Z","shell.execute_reply.started":"2025-08-06T11:34:16.027136Z","shell.execute_reply":"2025-08-06T11:34:16.060257Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preprocessing","metadata":{}},{"cell_type":"code","source":"green = pd.DataFrame([{\"ClassId\": \"43\", \"SignName\": \"Green\"}])\nyellow = pd.DataFrame([{\"ClassId\": \"44\", \"SignName\": \"Yellow\"}])\nred = pd.DataFrame([{\"ClassId\": \"45\", \"SignName\": \"Red\"}])\n\ndf = pd.concat([df, green], ignore_index=True)\ndf = pd.concat([df, yellow], ignore_index=True)\ndf = pd.concat([df, red], ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:34:16.061351Z","iopub.execute_input":"2025-08-06T11:34:16.061502Z","iopub.status.idle":"2025-08-06T11:34:16.068212Z","shell.execute_reply.started":"2025-08-06T11:34:16.061490Z","shell.execute_reply":"2025-08-06T11:34:16.067522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:34:16.068927Z","iopub.execute_input":"2025-08-06T11:34:16.069105Z","iopub.status.idle":"2025-08-06T11:34:16.081951Z","shell.execute_reply.started":"2025-08-06T11:34:16.069091Z","shell.execute_reply":"2025-08-06T11:34:16.081219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(43, 46):\n    os.makedirs(os.path.join(train_dir, str(i)), exist_ok=True)\n    os.makedirs(os.path.join(test_dir, str(i)), exist_ok=True)\n    os.makedirs(os.path.join(val_dir, str(i)), exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:34:16.086028Z","iopub.execute_input":"2025-08-06T11:34:16.086237Z","iopub.status.idle":"2025-08-06T11:34:16.091363Z","shell.execute_reply.started":"2025-08-06T11:34:16.086217Z","shell.execute_reply":"2025-08-06T11:34:16.090695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"green_train = \"/kaggle/input/carla-traffic-lights-images/traffic_light_data/train/green\"\nyellow_train = \"/kaggle/input/carla-traffic-lights-images/traffic_light_data/train/yellow\"\nred_train = \"/kaggle/input/carla-traffic-lights-images/traffic_light_data/train/red\"\n\ngreen_val = \"/kaggle/input/carla-traffic-lights-images/traffic_light_data/val/green\"\nyellow_val = \"/kaggle/input/carla-traffic-lights-images/traffic_light_data/val/yellow\"\nred_val = \"/kaggle/input/carla-traffic-lights-images/traffic_light_data/val/red\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:34:16.092077Z","iopub.execute_input":"2025-08-06T11:34:16.092330Z","iopub.status.idle":"2025-08-06T11:34:16.101881Z","shell.execute_reply.started":"2025-08-06T11:34:16.092306Z","shell.execute_reply":"2025-08-06T11:34:16.101189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_and_save_images(input_dir, output_dirs, class_id, filename_suffix):\n    files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))]\n    \n    for i, f in enumerate(tqdm(files, desc=f\"Processing {input_dir}\")):\n        img_path = os.path.join(input_dir, f)\n        img = cv2.imread(img_path)\n\n        resized_img = cv2.resize(img, (32, 32))\n        processed_img = process_image(resized_img)\n        \n        for out_dir in output_dirs:\n            out_path = os.path.join(out_dir, class_id, f\"{i}_{filename_suffix}.png\")\n            cv2.imwrite(out_path, processed_img)\n\nconfigs = [\n    # train\n    (green_train, [train_dir], \"43\", \"43\"),\n    (yellow_train, [train_dir], \"44\", \"44\"),\n    (red_train, [train_dir], \"45\", \"45\"),\n    \n    # validation/test\n    (green_val, [val_dir, test_dir], \"43\", \"43\"),\n    (yellow_val, [val_dir, test_dir], \"44\", \"44\"),\n    (red_val, [val_dir, test_dir], \"45\", \"45\"),\n]\n\nfor input_dir, output_dirs, class_id, suffix in configs:\n    process_and_save_images(input_dir, output_dirs, class_id, suffix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:34:16.102623Z","iopub.execute_input":"2025-08-06T11:34:16.102884Z","iopub.status.idle":"2025-08-06T11:34:30.765463Z","shell.execute_reply.started":"2025-08-06T11:34:16.102867Z","shell.execute_reply":"2025-08-06T11:34:30.764735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show(get_random_cv2_img(os.path.join(train_dir, \"43\")))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:34:30.766130Z","iopub.execute_input":"2025-08-06T11:34:30.766356Z","iopub.status.idle":"2025-08-06T11:34:30.802219Z","shell.execute_reply.started":"2025-08-06T11:34:30.766339Z","shell.execute_reply":"2025-08-06T11:34:30.801563Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train the classificator","metadata":{}},{"cell_type":"code","source":"%%capture\n! pip install ultralytics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:34:30.802939Z","iopub.execute_input":"2025-08-06T11:34:30.803506Z","iopub.status.idle":"2025-08-06T11:35:42.950152Z","shell.execute_reply.started":"2025-08-06T11:34:30.803488Z","shell.execute_reply":"2025-08-06T11:35:42.949316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Clear cache\ntorch.cuda.empty_cache()\n\n# Optional: Collect unused memory from Python garbage collector\nimport gc\ngc.collect()\n\n# If using newer PyTorch and CUDA version:\ntorch.cuda.ipc_collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T12:21:28.121076Z","iopub.execute_input":"2025-08-06T12:21:28.121362Z","iopub.status.idle":"2025-08-06T12:21:28.379222Z","shell.execute_reply.started":"2025-08-06T12:21:28.121341Z","shell.execute_reply":"2025-08-06T12:21:28.378382Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Using the size \"s\", because it seems to be fast and accurate enought.","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO\nmodel = YOLO(\"yolo11s-cls.pt\")\n\ndata = \"dataset/\"\n\nresults = model.train(data=data, batch=32, epochs=30, imgsz=32, device=[0, 1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T11:38:35.542391Z","iopub.execute_input":"2025-08-06T11:38:35.543182Z","iopub.status.idle":"2025-08-06T12:16:02.951920Z","shell.execute_reply.started":"2025-08-06T11:38:35.543156Z","shell.execute_reply":"2025-08-06T12:16:02.951232Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"%%capture\n! wget \"https://arcxyon.com/wp-content/uploads/2025/08/yolo-detect-m_best_epochs-100_size-460-960_05-08-2025.zip\"\n! unzip \"yolo-detect-m_best_epochs-100_size-460-960_05-08-2025.zip\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T12:19:18.506829Z","iopub.execute_input":"2025-08-06T12:19:18.507159Z","iopub.status.idle":"2025-08-06T12:19:21.463384Z","shell.execute_reply.started":"2025-08-06T12:19:18.507132Z","shell.execute_reply":"2025-08-06T12:19:21.462504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"detection_model = YOLO(\"yolo-detect-m_best_epochs-100_size-460-960_05-08-2025/yolo-detect-m_best_epochs-100_size-460-960_05-08-2025.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T12:19:40.580874Z","iopub.execute_input":"2025-08-06T12:19:40.581177Z","iopub.status.idle":"2025-08-06T12:19:40.696711Z","shell.execute_reply.started":"2025-08-06T12:19:40.581146Z","shell.execute_reply":"2025-08-06T12:19:40.696067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def refined_class(img):\n    # predict using the model (on CUDA device 0)\n    results = model.predict(img, device='cuda:0', batch=1)\n\n    # get the top-1 prediction (assuming single image input)\n    pred = results[0]  # result for the first (and only) image\n    cls = pred.names[pred.probs.top1]  # class label\n    conf = pred.probs.top1conf.item()  # confidence score (float)\n\n    return cls, conf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T12:19:42.747094Z","iopub.execute_input":"2025-08-06T12:19:42.747372Z","iopub.status.idle":"2025-08-06T12:19:42.752038Z","shell.execute_reply.started":"2025-08-06T12:19:42.747350Z","shell.execute_reply":"2025-08-06T12:19:42.751392Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Note: Make sure to preprocess the image the same as the training data and convert them to RGB**","metadata":{}},{"cell_type":"code","source":"def get_road_objects(img):\n\n    working_img = img.copy()\n    \n    # prprocess the images like the training data\n    working_img = process_image(working_img)\n\n    # make sure to convert to RGB because it was trained on RGB images\n    working_img = cv2.cvtColor(working_img, cv2.COLOR_BGR2RGB)\n    # use model.predict on specific device when having multiple gpus\n    results = detection_model.predict(working_img, device='cuda:0', batch=1)\n    \n    # assuming only one image processed per call\n    for result in results:\n        xyxy = result.boxes.xyxy  # [x1, y1, x2, y2]\n        class_ids = result.boxes.cls.int()\n        confs = result.boxes.conf\n        names = [result.names[cls.item()] for cls in class_ids]\n\n        for i, box in enumerate(xyxy):\n            x1, y1, x2, y2 = map(int, box.tolist())\n            cls_id = class_ids[i].item()\n            conf = confs[i]\n\n            # if it's class 2 (vehicle), skip get_class() and draw it directly\n            if cls_id == 2:\n                label = f\"{names[i]} {conf:.2f}\"\n            else:\n                # crop the image part\n                img_part = img[y1:y2, x1:x2]\n                \n                # again, the classificator was trained on RGB images of size 32 by 32\n                img_part = cv2.cvtColor(img_part, cv2.COLOR_BGR2RGB)\n                img_part = cv2.resize(img_part, (32, 32))\n                img_part = process_image(img_part)\n                \n                rfc, conf_s = refined_class(img_part)\n                \n                if refined_class is None:\n                    continue  # Skip drawing if classification failed\n\n                sign_name = df.loc[df['ClassId'] == int(rfc), 'SignName'].values[0]\n                label = f\"{sign_name} {conf_s:.2f}\"\n\n            cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 0, 255), thickness=2)\n\n            cv2.putText(\n                img,\n                label,\n                (x1, max(y1 - 10, 0)),  # Ensure the text isn't drawn out of image bounds\n                fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n                fontScale=0.5,\n                color=(0, 0, 255),\n                thickness=1\n            )\n            \n    return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T12:28:59.749033Z","iopub.execute_input":"2025-08-06T12:28:59.749766Z","iopub.status.idle":"2025-08-06T12:28:59.757394Z","shell.execute_reply.started":"2025-08-06T12:28:59.749742Z","shell.execute_reply":"2025-08-06T12:28:59.756622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n! wget \"https://arcxyon.com/wp-content/uploads/2025/08/Raw_Examples_04-08-2025.zip\"\n! unzip \"Raw_Examples_04-08-2025.zip\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T12:20:17.912003Z","iopub.execute_input":"2025-08-06T12:20:17.912259Z","iopub.status.idle":"2025-08-06T12:20:20.004029Z","shell.execute_reply.started":"2025-08-06T12:20:17.912242Z","shell.execute_reply":"2025-08-06T12:20:20.002977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_img_path = \"67/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T12:20:28.405814Z","iopub.execute_input":"2025-08-06T12:20:28.406119Z","iopub.status.idle":"2025-08-06T12:20:28.410499Z","shell.execute_reply.started":"2025-08-06T12:20:28.406094Z","shell.execute_reply":"2025-08-06T12:20:28.409785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_name = get_random_cv2_img(example_img_path, True)\nimg_path = os.path.join(example_img_path, img_name)\nimg = cv2.imread(img_path)\n\nshow(get_road_objects(img), cv2_img=True, size=(12, 10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T12:29:19.406074Z","iopub.execute_input":"2025-08-06T12:29:19.406694Z","iopub.status.idle":"2025-08-06T12:29:19.847223Z","shell.execute_reply.started":"2025-08-06T12:29:19.406669Z","shell.execute_reply":"2025-08-06T12:29:19.846565Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ","metadata":{}}]}